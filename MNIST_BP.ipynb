{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        # 作用：初始化w和b\n",
    "        # param sizes: list 类型，存储每层神经网络的神经元数目\n",
    "        # 例如sizes=[2,3,2]表示输出层有两个神经元，隐藏层有3个神经元，输出层有2个神经元\n",
    "        \n",
    "        # num_layers代表神经网络数目\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        # 初始输入层，随机产生每层中的y个神经元的biase值（0,1）\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x) for x, y in zip(sizes[:-1],sizes[1:])]\n",
    "        \n",
    "        # biases和weights是共用的参数。只有一套\n",
    "        \n",
    "        # biases和weights其实是一个不规则的二维数组。如果输入是3层，那么数组有2行\n",
    "        # 第一行30个维度，第二行10个维度\n",
    "        \n",
    "        # 这个weights的初始方法算是学到了。假设sizes是[a1,a2,a3,a4]\n",
    "        # 那么输出是（a1,a2）(a2,a3) (a3,a4)\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        # 作用：向前传输计算每个神经元的值\n",
    "        # param a：输入值\n",
    "        # return：计算后每个神经元的值\n",
    "        \n",
    "        a = x.reshape(x.shape[0],1) # 将数据由(x,)转换成(x,1)。否则矩阵计算会出错\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            # 加权求和再加上biase\n",
    "            a = sigmoid(np.dot(w,a)+b)\n",
    "            # dot是矩阵乘法的意思\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, x_train, y_train, x_test, y_test, epochs, mini_batch_size, eta):\n",
    "        # 作用：随机梯度下降\n",
    "        # param training_data：输入的训练集\n",
    "        # param epochs：迭代次数\n",
    "        # param mini_batch_size：小样本数量\n",
    "        # param eta：学习率\n",
    "        # param test_data：测试数据集\n",
    "\n",
    "        n_test = x_test.shape[0]\n",
    "        n_train =  x_train.shape[0] # n_train是总的训练样本数目\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            # xrange和range类似，但是是返回一个迭代器，性能更优。但是在python3中，xrange和range是合并了的\n",
    "            # xrange(start, stop[, step])\n",
    "            \n",
    "            # 搅乱训练集，让其排序顺序发生变化            \n",
    "            temp = list(zip(x_train, y_train))\n",
    "            random.shuffle(temp)\n",
    "            x_train[:], y_train[:] = zip(*temp)\n",
    "            \n",
    "            # 按照小样本数量划分训练集          \n",
    "            mini_batches_x = [\n",
    "                x_train[k:k+mini_batch_size]\n",
    "                for k in range(0, n_train, mini_batch_size)\n",
    "            ]\n",
    "            mini_batches_y = [\n",
    "                y_train[k:k+mini_batch_size]\n",
    "                for k in range(0, n_train, mini_batch_size)\n",
    "            ]\n",
    "            # print(\"mini_batches_x=\", np.array(mini_batches_x).shape)\n",
    "            # print(\"mini_batches_y= \", np.array(mini_batches_y).shape)\n",
    "            for (mini_x,mini_y) in zip(mini_batches_x,mini_batches_y):\n",
    "                # 根据每个小样本来更新w和b\n",
    "                self.updata_mini_batch(mini_x,mini_y,eta)\n",
    "            \n",
    "            # 输出测试每轮结束后，神经网络的准确度\n",
    "            if True:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(x_test,y_test), n_test))\n",
    "                #pdb.set_trace()\n",
    "            else:\n",
    "                # 若没有数据集的情况下，则表示该次训练结束？\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "                \n",
    "    def updata_mini_batch(self, mini_x, mini_y, eta):\n",
    "        # 作用：根据每个小样本来更新w和b\n",
    "        # param mini_batch：一部分的样本\n",
    "        # parma eta：学习率\n",
    "        \n",
    "        # 根据b和w的大小分别创建全为0的矩阵\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for (x, y) in zip(mini_x, mini_y):\n",
    "            # 根据样本中的每一个输入x的其输出y，计算w和b的偏导数\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x,y)\n",
    "            # 累加存储偏导值\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "        # 根据累加的偏导值更新w和b，这里因为使用了小样本，所以eta要除以小样本的长度\n",
    "        self.weights = [w-(eta/mini_x.shape[0])*nw\n",
    "                       for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/mini_x.shape[0])*nb\n",
    "                      for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        # 作用：计算偏导数\n",
    "        # param x：输入一个样本\n",
    "        # param y：样本对应的值\n",
    "        # return：b，w\n",
    "        \n",
    "        ##print(\"x= \", x.shape)\n",
    "        ##print(\"y= \",y.shape)  \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # 前向传输\n",
    "        activation = x.reshape(x.shape[0], 1) # 将数据由(x,)转换成(x,1)。否则矩阵计算会出错\n",
    "        # 存储每层的神经元的值的矩阵\n",
    "        activations = [x]\n",
    "        # 存储每个未经过sigmoid计算的神经元的值\n",
    "        zs = []\n",
    "        for b,w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # 求 δ 的值\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        # 乘于前一层的输出值\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            # 从倒数第l层开始更新\n",
    "            # 下面这里利用 l+1 层的 δ 值来计算 l 的 δ 值\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].reshape(1, activations[-l-1].shape[0])) # 将数据由(x,)转换成(x,1)。否则矩阵计算会出错\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        # 获得预测结果\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in zip(x_test, y_test)]\n",
    "        # 返回正确识别的个数\n",
    "        # print(test_results)\n",
    "        res = sum(int(x == y) for (x, y) in test_results)\n",
    "        print(\"This is the result: \", res)\n",
    "        return res\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):    \n",
    "        # 作用：二次损失函数\n",
    "        # :param output_activations:\n",
    "        # :param y: 一个数字，如6\n",
    "        # :return: \n",
    "        \n",
    "        e = np.zeros((10,1)) # 矩阵间的减法，需要对齐。在相应数字的位置置1，其余是0\n",
    "        e[y] = 1.0\n",
    "\n",
    "        return (output_activations-e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    #作用：求 sigmoid 函数的值\n",
    "    # param z:\n",
    "    # return:\n",
    "    \n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    # 作用：求 sigmoid 函数的导数\n",
    "    # :param z:\n",
    "    # :return:\n",
    "    \n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 1)\n",
      "(10000, 784)\n",
      "(10000, 1)\n",
      "load data finish\n",
      "> <ipython-input-10-84adde9c4133>(23)<module>()\n",
      "-> net = Network([784, 30, 10])\n",
      "(Pdb) c\n",
      "net.w1=  (30, 1)\n",
      "net.w2=  (10, 1)\n",
      "net.w1=  (30, 784)\n",
      "net.w2=  (10, 30)\n",
      "This is the result:  7983\n",
      "Epoch 0: 7983 / 10000\n",
      "This is the result:  9144\n",
      "Epoch 1: 9144 / 10000\n",
      "This is the result:  9276\n",
      "Epoch 2: 9276 / 10000\n",
      "This is the result:  9296\n",
      "Epoch 3: 9296 / 10000\n",
      "This is the result:  9285\n",
      "Epoch 4: 9285 / 10000\n",
      "This is the result:  9300\n",
      "Epoch 5: 9300 / 10000\n",
      "This is the result:  9296\n",
      "Epoch 6: 9296 / 10000\n",
      "This is the result:  9303\n",
      "Epoch 7: 9303 / 10000\n",
      "This is the result:  9289\n",
      "Epoch 8: 9289 / 10000\n",
      "This is the result:  9291\n",
      "Epoch 9: 9291 / 10000\n",
      "This is the result:  9278\n",
      "Epoch 10: 9278 / 10000\n",
      "This is the result:  9289\n",
      "Epoch 11: 9289 / 10000\n",
      "This is the result:  9288\n",
      "Epoch 12: 9288 / 10000\n",
      "This is the result:  9278\n",
      "Epoch 13: 9278 / 10000\n",
      "This is the result:  9273\n",
      "Epoch 14: 9273 / 10000\n",
      "This is the result:  9275\n",
      "Epoch 15: 9275 / 10000\n",
      "This is the result:  9277\n",
      "Epoch 16: 9277 / 10000\n",
      "This is the result:  9264\n",
      "Epoch 17: 9264 / 10000\n",
      "This is the result:  9267\n",
      "Epoch 18: 9267 / 10000\n",
      "This is the result:  9264\n",
      "Epoch 19: 9264 / 10000\n",
      "This is the result:  9267\n",
      "Epoch 20: 9267 / 10000\n",
      "This is the result:  9260\n",
      "Epoch 21: 9260 / 10000\n",
      "This is the result:  9254\n",
      "Epoch 22: 9254 / 10000\n",
      "This is the result:  9253\n",
      "Epoch 23: 9253 / 10000\n",
      "This is the result:  9258\n",
      "Epoch 24: 9258 / 10000\n",
      "This is the result:  9262\n",
      "Epoch 25: 9262 / 10000\n",
      "This is the result:  9260\n",
      "Epoch 26: 9260 / 10000\n",
      "This is the result:  9260\n",
      "Epoch 27: 9260 / 10000\n",
      "This is the result:  9263\n",
      "Epoch 28: 9263 / 10000\n",
      "This is the result:  9257\n",
      "Epoch 29: 9257 / 10000\n"
     ]
    }
   ],
   "source": [
    "# %pdb on\n",
    "if __name__  ==   \"__main__\":\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data() # 加载数据集，原本的数据格式是60000*28*28。三维的形式\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])\n",
    "    y_train = y_train.reshape(y_train.shape[0],1)\n",
    "    y_test = y_test.reshape(y_test.shape[0],1)\n",
    "    x_train = x_train.astype('float32') # 将数据类型转为float32，因为很多时候我们用numpy从文本文件读取数据作为numpy的数组，默认的dtype是float64\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    print(x_train.shape) # (60000, 784)\n",
    "    print(y_train.shape) # (60000, 1)\n",
    "    print(x_test.shape) # (10000, 784)\n",
    "    print(y_test.shape) # (10000, 1)\n",
    "    print(\"load data finish\")\n",
    "    \n",
    "    pdb.set_trace() # 设置中断，输入c继续运行\n",
    "    \n",
    "    net = Network([784, 30, 10])\n",
    "    print(\"net.w1= \", np.array(net.biases[0]).shape)\n",
    "    print(\"net.w2= \", np.array(net.biases[1]).shape)\n",
    "    print(\"net.w1= \", np.array(net.weights[0]).shape)\n",
    "    print(\"net.w2= \", np.array(net.weights[1]).shape)\n",
    "    net.SGD(x_train, y_train, x_test, y_test, 30, 10, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test1]",
   "language": "python",
   "name": "conda-env-test1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
